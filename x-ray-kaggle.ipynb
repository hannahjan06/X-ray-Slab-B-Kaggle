{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('/kaggle/input/grand-xray-slam-division-b/train2.csv')\n",
    "\n",
    "data_df['Image_path'] = '/kaggle/input/grand-xray-slam-division-b/train2/' + data_df['Image_name']\n",
    "cols = ['Image_path'] + [c for c in data_df.columns if c != 'Image_path']\n",
    "data_df = data_df[cols]\n",
    "\n",
    "broken_images = ['/kaggle/input/grand-xray-slam-division-b/train2/00043046_001_001.jpg',\n",
    " '/kaggle/input/grand-xray-slam-division-b/train2/00052495_001_001.jpg',\n",
    " '/kaggle/input/grand-xray-slam-division-b/train2/00056890_001_001.jpg']\n",
    "data_df = data_df[~data_df['Image_path'].isin(broken_images)].reset_index(drop=True)\n",
    "\n",
    "data_df = data_df.drop('Image_name', axis=1)\n",
    "data_df = data_df.drop('Study', axis=1)\n",
    "data_df = data_df.drop('Patient_ID', axis=1)\n",
    "\n",
    "data_df['Age'] = data_df['Age'].fillna(data_df['Age'].mean())\n",
    "data_df['Sex'] = data_df['Sex'].fillna(data_df['Sex'].mode()[0])\n",
    "\n",
    "data_df.columns = data_df.columns.str.replace(' ','_')\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:39.076200Z",
     "iopub.status.busy": "2025-10-09T06:44:39.075720Z",
     "iopub.status.idle": "2025-10-09T06:44:45.020525Z",
     "shell.execute_reply": "2025-10-09T06:44:45.019709Z",
     "shell.execute_reply.started": "2025-10-09T06:44:39.076176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should be >=1\n",
    "print(torch.cuda.get_device_name(0))  # Name of GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:45.022178Z",
     "iopub.status.busy": "2025-10-09T06:44:45.021747Z",
     "iopub.status.idle": "2025-10-09T06:44:45.028226Z",
     "shell.execute_reply": "2025-10-09T06:44:45.027548Z",
     "shell.execute_reply.started": "2025-10-09T06:44:45.022159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "for cols in data_df.columns:\n",
    "    if data_df[cols].dtype == np.float64 or data_df[cols].dtype == np.int64:\n",
    "        num_cols.append(cols)\n",
    "\n",
    "print('Numerical columns: ',num_cols, '\\n')\n",
    "\n",
    "cat_cols = []\n",
    "for cols in data_df.columns:\n",
    "    if data_df[cols].dtype == object:\n",
    "        if cols == 'Image_path':\n",
    "            continue\n",
    "        else:\n",
    "            cat_cols.append(cols)\n",
    "\n",
    "print('Object columns: ',cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T06:44:47.876037Z",
     "iopub.status.busy": "2025-10-09T06:44:47.875363Z",
     "iopub.status.idle": "2025-10-09T09:57:05.718893Z",
     "shell.execute_reply": "2025-10-09T09:57:05.716620Z",
     "shell.execute_reply.started": "2025-10-09T06:44:47.876013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# GPU optimization settings\n",
    "tf.config.optimizer.set_jit(True)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=15000)])\n",
    "        print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# ------------------- CONFIGURATION -------------------\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Paths to your saved weights (if available)\n",
    "WEIGHTS_PATHS = {\n",
    "    'resnet50': '/kaggle/input/resnet-model/tensorflow2/default/1/best_resnet_model.h5',\n",
    "    'efficientnet': None,  # Set to your path if available\n",
    "    'densenet': None       # Set to your path if available\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "              'Enlarged_Cardiomediastinum', 'Fracture', 'Lung_Lesion', \n",
    "              'Lung_Opacity', 'No_Finding', 'Pleural_Effusion', 'Pleural_Other', \n",
    "              'Pneumonia', 'Pneumothorax', 'Support_Devices']\n",
    "\n",
    "# Assume data_df is already loaded\n",
    "print(f\"Using dataframe with {len(data_df)} samples\")\n",
    "\n",
    "train_seqs, val_seqs = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "train_seqs = train_seqs.reset_index(drop=True)\n",
    "val_seqs = val_seqs.reset_index(drop=True)\n",
    "\n",
    "# threshold = 20% of total samples\n",
    "small_classes = [\n",
    "    col for col in label_cols \n",
    "    if train_seqs[col].sum() / len(data_df) < 0.2\n",
    "]\n",
    "\n",
    "print(\"Small classes:\", small_classes)\n",
    "\n",
    "# Compute label weights\n",
    "class_counts = data_df[label_cols].sum().values\n",
    "total_samples = len(data_df)\n",
    "pos_weights = np.maximum(class_counts, 1)\n",
    "neg_weights = total_samples - pos_weights\n",
    "label_weights = neg_weights / pos_weights\n",
    "label_weights = np.clip(label_weights, 0.5, 3.0)\n",
    "label_weights = tf.constant(label_weights, dtype=tf.float32)\n",
    "\n",
    "print(\"Label weights:\", label_weights.numpy())\n",
    "\n",
    "# ------------------- LOSS FUNCTION -------------------\n",
    "@tf.function(jit_compile=True)\n",
    "def weighted_bce(y_true, y_pred):\n",
    "    \"\"\"Stable weighted binary crossentropy\"\"\"\n",
    "    bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    weighted_bce = bce * label_weights\n",
    "    return tf.reduce_mean(weighted_bce)\n",
    "\n",
    "# ------------------- DATA PIPELINE -------------------\n",
    "@tf.function\n",
    "def decode_and_process_train(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
    "\n",
    "    # Base augmentations\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    if tf.random.uniform([]) < 0.5:\n",
    "        k = tf.random.uniform([], -1, 2, dtype=tf.int32)\n",
    "        img = tf.image.rot90(img, k=k)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.image.random_contrast(img, 0.85, 1.15)\n",
    "\n",
    "    # Extra augmentations for smaller classes\n",
    "    if tf.reduce_any(tf.greater(tf.gather(label, \n",
    "        [label_cols.index(c) for c in small_classes]), 0)):\n",
    "        img = tf.image.random_saturation(img, 0.8, 1.3)\n",
    "        img = tf.image.random_hue(img, 0.05)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        if tf.random.uniform([]) < 0.3:\n",
    "            noise = tf.random.normal(tf.shape(img), mean=0.0, stddev=5.0)\n",
    "            img = tf.clip_by_value(img + noise, 0, 255)\n",
    "\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    return img, label\n",
    "\n",
    "@tf.function\n",
    "def decode_and_process_val(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    return img, label\n",
    "\n",
    "# Prepare data\n",
    "train_paths = train_seqs['Image_path'].values\n",
    "train_labels = train_seqs[label_cols].values.astype(\"float32\")\n",
    "val_paths = val_seqs['Image_path'].values\n",
    "val_labels = val_seqs[label_cols].values.astype(\"float32\")\n",
    "\n",
    "print(f\"Training samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "\n",
    "# Build datasets\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    .shuffle(3000, reshuffle_each_iteration=True)\n",
    "    .map(decode_and_process_train, num_parallel_calls=AUTOTUNE, deterministic=False)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=3)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    .map(decode_and_process_val, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=2)\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(train_paths) // BATCH_SIZE}\")\n",
    "\n",
    "# ------------------- BUILD INDIVIDUAL MODELS -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Building Ensemble Models...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Mixed precision\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"Mixed precision: {policy.name}\")\n",
    "\n",
    "def build_model(base_architecture, name):\n",
    "    \"\"\"Build a model with the given base architecture\"\"\"\n",
    "    if base_architecture == 'resnet50':\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False,\n",
    "            input_shape=IMG_SIZE + (3,),\n",
    "            weights=None\n",
    "        )\n",
    "    elif base_architecture == 'efficientnet':\n",
    "        base_model = tf.keras.applications.EfficientNetB3(\n",
    "            include_top=False,\n",
    "            input_shape=IMG_SIZE + (3,),\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    elif base_architecture == 'densenet':\n",
    "        base_model = tf.keras.applications.DenseNet121(\n",
    "            include_top=False,\n",
    "            input_shape=IMG_SIZE + (3,),\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    \n",
    "    # Build head\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    logits = tf.keras.layers.Dense(\n",
    "        len(label_cols),\n",
    "        activation=None,\n",
    "        dtype='float32',\n",
    "        name=f'{name}_output'\n",
    "    )(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=logits, name=name)\n",
    "    \n",
    "    # Make all layers trainable\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the three models\n",
    "model_resnet = build_model('resnet50', 'resnet50')\n",
    "model_efficientnet = build_model('efficientnet', 'efficientnet')\n",
    "model_densenet = build_model('densenet', 'densenet')\n",
    "\n",
    "models_list = [model_resnet, model_efficientnet, model_densenet]\n",
    "model_names = ['resnet50', 'efficientnet', 'densenet']\n",
    "\n",
    "# Load pre-trained weights if available\n",
    "if WEIGHTS_PATHS['resnet50'] and os.path.exists(WEIGHTS_PATHS['resnet50']):\n",
    "    model_resnet.load_weights(WEIGHTS_PATHS['resnet50'])\n",
    "    print(f\"✓ Loaded ResNet50 weights from: {WEIGHTS_PATHS['resnet50']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Summary:\")\n",
    "for model, name in zip(models_list, model_names):\n",
    "    trainable_count = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"{name}: {trainable_count:,} trainable parameters\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ------------------- BUILD ENSEMBLE MODEL -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Building Ensemble Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Input layer\n",
    "input_layer = tf.keras.Input(shape=IMG_SIZE + (3,), name='ensemble_input')\n",
    "\n",
    "# Get predictions from all three models\n",
    "pred1 = model_resnet(input_layer)\n",
    "pred2 = model_efficientnet(input_layer)\n",
    "pred3 = model_densenet(input_layer)\n",
    "\n",
    "# Average predictions\n",
    "ensemble_output = tf.keras.layers.Average(name='ensemble_average')([pred1, pred2, pred3])\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble_model = tf.keras.Model(inputs=input_layer, outputs=ensemble_output, name='ensemble')\n",
    "\n",
    "print(\"✓ Ensemble model created\")\n",
    "print(f\"Total models in ensemble: {len(models_list)}\")\n",
    "\n",
    "# ------------------- COMPILE ENSEMBLE MODEL -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Compiling ensemble model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fine_tune_lr = 1e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=fine_tune_lr,\n",
    "    weight_decay=1e-6,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_bce,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(name=\"auc\", from_logits=True),\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5)\n",
    "    ],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Compiled with learning rate: {fine_tune_lr:.2e}\")\n",
    "\n",
    "# ------------------- CALLBACKS -------------------\n",
    "# Clean up any existing checkpoint files\n",
    "checkpoint_path = 'ensemble_model.weights.h5'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "    print(f\"Removed existing checkpoint: {checkpoint_path}\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,  # Save weights only to avoid HDF5 issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        import time\n",
    "        self.epoch_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        import time\n",
    "        epoch_time = time.time() - self.epoch_start\n",
    "        print(f\"\\nEpoch time: {epoch_time/60:.1f} minutes\")\n",
    "\n",
    "timing_cb = TimingCallback()\n",
    "\n",
    "# ------------------- TRAIN ENSEMBLE -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training ensemble model...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "history = ensemble_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, timing_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------- RESULTS -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Ensemble training completed!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best validation AUC: {max(history.history['val_auc']):.4f}\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final learning rate: {ensemble_model.optimizer.learning_rate.numpy():.2e}\")\n",
    "\n",
    "# Save final ensemble model\n",
    "final_model_path = 'final_ensemble_model.h5'\n",
    "if os.path.exists(final_model_path):\n",
    "    os.remove(final_model_path)\n",
    "ensemble_model.save_weights(final_model_path)\n",
    "print(f\"\\n✓ Ensemble model weights saved to: {final_model_path}\")\n",
    "\n",
    "# ------------------- INFERENCE FUNCTION -------------------\n",
    "def predict_ensemble(image_paths):\n",
    "    \"\"\"\n",
    "    Make predictions using the ensemble model\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Probabilities for each class (sigmoid applied to logits)\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        .map(lambda x: decode_and_process_val(x, tf.zeros(len(label_cols))))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(buffer_size=2)\n",
    "    )\n",
    "    \n",
    "    # Get logits from ensemble\n",
    "    logits = ensemble_model.predict(ds, verbose=1)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    predictions = tf.sigmoid(logits).numpy()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Use predict_ensemble(image_paths) for inference\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optional: Clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normal ensemble finetune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T09:58:37.252475Z",
     "iopub.status.busy": "2025-10-09T09:58:37.251761Z",
     "iopub.status.idle": "2025-10-09T11:28:51.238163Z",
     "shell.execute_reply": "2025-10-09T11:28:51.236521Z",
     "shell.execute_reply.started": "2025-10-09T09:58:37.252450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# GPU optimization settings\n",
    "tf.config.optimizer.set_jit(True)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=15000)])\n",
    "        print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# ------------------- CONFIGURATION -------------------\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32  # Smaller batch size for fine-tuning\n",
    "EPOCHS = 5  # Fewer epochs for fine-tuning\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Path to your trained ensemble weights\n",
    "ENSEMBLE_WEIGHTS_PATH = 'ensemble_model.weights.h5'\n",
    "\n",
    "# Number of layers to unfreeze from the top of each base model\n",
    "UNFREEZE_LAYERS = 50  # Adjust this: higher = more layers unfrozen\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "              'Enlarged_Cardiomediastinum', 'Fracture', 'Lung_Lesion', \n",
    "              'Lung_Opacity', 'No_Finding', 'Pleural_Effusion', 'Pleural_Other', \n",
    "              'Pneumonia', 'Pneumothorax', 'Support_Devices']\n",
    "\n",
    "# Assume data_df is already loaded\n",
    "print(f\"Using dataframe with {len(data_df)} samples\")\n",
    "\n",
    "train_seqs, val_seqs = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "train_seqs = train_seqs.reset_index(drop=True)\n",
    "val_seqs = val_seqs.reset_index(drop=True)\n",
    "\n",
    "# threshold = 20% of total samples\n",
    "small_classes = [\n",
    "    col for col in label_cols \n",
    "    if train_seqs[col].sum() / len(data_df) < 0.2\n",
    "]\n",
    "\n",
    "print(\"Small classes:\", small_classes)\n",
    "\n",
    "# Compute label weights\n",
    "class_counts = data_df[label_cols].sum().values\n",
    "total_samples = len(data_df)\n",
    "pos_weights = np.maximum(class_counts, 1)\n",
    "neg_weights = total_samples - pos_weights\n",
    "label_weights = neg_weights / pos_weights\n",
    "label_weights = np.clip(label_weights, 0.5, 3.0)\n",
    "label_weights = tf.constant(label_weights, dtype=tf.float32)\n",
    "\n",
    "print(\"Label weights:\", label_weights.numpy())\n",
    "\n",
    "# ------------------- LOSS FUNCTION -------------------\n",
    "@tf.function(jit_compile=True)\n",
    "def weighted_bce(y_true, y_pred):\n",
    "    \"\"\"Stable weighted binary crossentropy\"\"\"\n",
    "    bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    weighted_bce = bce * label_weights\n",
    "    return tf.reduce_mean(weighted_bce)\n",
    "\n",
    "# ------------------- DATA PIPELINE -------------------\n",
    "@tf.function\n",
    "def decode_and_process_train(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
    "\n",
    "    # Base augmentations\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    if tf.random.uniform([]) < 0.5:\n",
    "        k = tf.random.uniform([], -1, 2, dtype=tf.int32)\n",
    "        img = tf.image.rot90(img, k=k)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.image.random_contrast(img, 0.85, 1.15)\n",
    "\n",
    "    # Extra augmentations for smaller classes\n",
    "    if tf.reduce_any(tf.greater(tf.gather(label, \n",
    "        [label_cols.index(c) for c in small_classes]), 0)):\n",
    "        img = tf.image.random_saturation(img, 0.8, 1.3)\n",
    "        img = tf.image.random_hue(img, 0.05)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        if tf.random.uniform([]) < 0.3:\n",
    "            noise = tf.random.normal(tf.shape(img), mean=0.0, stddev=5.0)\n",
    "            img = tf.clip_by_value(img + noise, 0, 255)\n",
    "\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    return img, label\n",
    "\n",
    "@tf.function\n",
    "def decode_and_process_val(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    return img, label\n",
    "\n",
    "# Prepare data\n",
    "train_paths = train_seqs['Image_path'].values\n",
    "train_labels = train_seqs[label_cols].values.astype(\"float32\")\n",
    "val_paths = val_seqs['Image_path'].values\n",
    "val_labels = val_seqs[label_cols].values.astype(\"float32\")\n",
    "\n",
    "print(f\"Training samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "\n",
    "# Build datasets\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    .shuffle(3000, reshuffle_each_iteration=True)\n",
    "    .map(decode_and_process_train, num_parallel_calls=AUTOTUNE, deterministic=False)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=3)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    .map(decode_and_process_val, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=2)\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(train_paths) // BATCH_SIZE}\")\n",
    "\n",
    "# ------------------- REBUILD ENSEMBLE MODEL -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Rebuilding Ensemble Architecture...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Mixed precision\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"Mixed precision: {policy.name}\")\n",
    "\n",
    "def build_model(base_architecture, name):\n",
    "    \"\"\"Build a model with the given base architecture\"\"\"\n",
    "    if base_architecture == 'resnet50':\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False,\n",
    "            input_shape=IMG_SIZE + (3,),\n",
    "            weights=None\n",
    "        )\n",
    "    elif base_architecture == 'efficientnet':\n",
    "        base_model = tf.keras.applications.EfficientNetB3(\n",
    "            include_top=False,\n",
    "            input_shape=IMG_SIZE + (3,),\n",
    "            weights=None\n",
    "        )\n",
    "    elif base_architecture == 'densenet':\n",
    "        base_model = tf.keras.applications.DenseNet121(\n",
    "            include_top=False,\n",
    "            input_shape=IMG_SIZE + (3,),\n",
    "            weights=None\n",
    "        )\n",
    "    \n",
    "    # Build head\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    logits = tf.keras.layers.Dense(\n",
    "        len(label_cols),\n",
    "        activation=None,\n",
    "        dtype='float32',\n",
    "        name=f'{name}_output'\n",
    "    )(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=logits, name=name)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build the three models\n",
    "model_resnet, base_resnet = build_model('resnet50', 'resnet50')\n",
    "model_efficientnet, base_efficientnet = build_model('efficientnet', 'efficientnet')\n",
    "model_densenet, base_densenet = build_model('densenet', 'densenet')\n",
    "\n",
    "models_list = [model_resnet, model_efficientnet, model_densenet]\n",
    "base_models_list = [base_resnet, base_efficientnet, base_densenet]\n",
    "model_names = ['resnet50', 'efficientnet', 'densenet']\n",
    "\n",
    "# Input layer\n",
    "input_layer = tf.keras.Input(shape=IMG_SIZE + (3,), name='ensemble_input')\n",
    "\n",
    "# Get predictions from all three models\n",
    "pred1 = model_resnet(input_layer)\n",
    "pred2 = model_efficientnet(input_layer)\n",
    "pred3 = model_densenet(input_layer)\n",
    "\n",
    "# Average predictions\n",
    "ensemble_output = tf.keras.layers.Average(name='ensemble_average')([pred1, pred2, pred3])\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble_model = tf.keras.Model(inputs=input_layer, outputs=ensemble_output, name='ensemble')\n",
    "\n",
    "print(\"✓ Ensemble architecture rebuilt\")\n",
    "\n",
    "# ------------------- LOAD TRAINED WEIGHTS -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Loading trained ensemble weights...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.path.exists(ENSEMBLE_WEIGHTS_PATH):\n",
    "    ensemble_model.load_weights(ENSEMBLE_WEIGHTS_PATH)\n",
    "    print(f\"✓ Loaded weights from: {ENSEMBLE_WEIGHTS_PATH}\")\n",
    "else:\n",
    "    print(f\"WARNING: Weights file not found at {ENSEMBLE_WEIGHTS_PATH}\")\n",
    "    print(\"Please check the path!\")\n",
    "    raise FileNotFoundError(f\"Weights not found: {ENSEMBLE_WEIGHTS_PATH}\")\n",
    "\n",
    "# ------------------- FREEZE STRATEGY -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Unfreezing top {UNFREEZE_LAYERS} layers of each base model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def unfreeze_top_layers(base_model, num_layers_to_unfreeze):\n",
    "    \"\"\"Unfreeze the top N layers of a model\"\"\"\n",
    "    # First freeze everything\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Then unfreeze the last N layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    layers_to_unfreeze = min(num_layers_to_unfreeze, total_layers)\n",
    "    \n",
    "    for layer in base_model.layers[-layers_to_unfreeze:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    trainable_count = sum([1 for layer in base_model.layers if layer.trainable])\n",
    "    return trainable_count\n",
    "\n",
    "# Unfreeze top layers for each base model\n",
    "print(\"\\nUnfreezing layers:\")\n",
    "for base_model, name in zip(base_models_list, model_names):\n",
    "    trainable = unfreeze_top_layers(base_model, UNFREEZE_LAYERS)\n",
    "    total = len(base_model.layers)\n",
    "    print(f\"  {name}: {trainable}/{total} layers trainable\")\n",
    "\n",
    "# The head layers (GlobalAveragePooling, Dropout, Dense) are always trainable\n",
    "print(\"\\nHead layers (GAP, Dropout, Dense) are always trainable\")\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_count = sum([tf.size(w).numpy() for w in ensemble_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(w).numpy() for w in ensemble_model.non_trainable_weights])\n",
    "total_count = trainable_count + non_trainable_count\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Trainable parameters: {trainable_count:,} ({100*trainable_count/total_count:.1f}%)\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "print(f\"Total parameters: {total_count:,}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ------------------- COMPILE WITH VERY LOW LEARNING RATE -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Compiling for fine-tuning...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# VERY low learning rate for fine-tuning top layers\n",
    "fine_tune_lr = 5e-6  # Even lower than before\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=fine_tune_lr,\n",
    "    weight_decay=1e-7,  # Lower weight decay\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_bce,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(name=\"auc\", from_logits=True),\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5)\n",
    "    ],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Compiled with learning rate: {fine_tune_lr:.2e}\")\n",
    "\n",
    "# ------------------- CALLBACKS -------------------\n",
    "# Clean up any existing checkpoint files\n",
    "checkpoint_path = 'finetuned_ensemble.weights.h5'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "    print(f\"Removed existing checkpoint: {checkpoint_path}\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-8\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        import time\n",
    "        self.epoch_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        import time\n",
    "        epoch_time = time.time() - self.epoch_start\n",
    "        print(f\"\\nEpoch time: {epoch_time/60:.1f} minutes\")\n",
    "\n",
    "timing_cb = TimingCallback()\n",
    "\n",
    "# ------------------- FINE-TUNE -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting fine-tuning with top layers unfrozen...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "history = ensemble_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, timing_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------- RESULTS -------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-tuning completed!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best validation AUC: {max(history.history['val_auc']):.4f}\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final learning rate: {ensemble_model.optimizer.learning_rate.numpy():.2e}\")\n",
    "\n",
    "# Save final fine-tuned model\n",
    "final_model_path = 'final_finetuned_ensemble.weights.h5'\n",
    "if os.path.exists(final_model_path):\n",
    "    os.remove(final_model_path)\n",
    "ensemble_model.save_weights(final_model_path)\n",
    "print(f\"\\n✓ Fine-tuned ensemble weights saved to: {final_model_path}\")\n",
    "\n",
    "# ------------------- INFERENCE FUNCTION -------------------\n",
    "def predict_ensemble(image_paths):\n",
    "    \"\"\"\n",
    "    Make predictions using the fine-tuned ensemble model\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Probabilities for each class (sigmoid applied to logits)\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        .map(lambda x: decode_and_process_val(x, tf.zeros(len(label_cols))))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(buffer_size=2)\n",
    "    )\n",
    "    \n",
    "    # Get logits from ensemble\n",
    "    logits = ensemble_model.predict(ds, verbose=1)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    predictions = tf.sigmoid(logits).numpy()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Use predict_ensemble(image_paths) for inference\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optional: Clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13471427,
     "sourceId": 113002,
     "sourceType": "competition"
    },
    {
     "modelId": 466236,
     "modelInstanceId": 449860,
     "sourceId": 600415,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 466714,
     "modelInstanceId": 450355,
     "sourceId": 601015,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
